{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1qu69Pu9TFLjjsK7YAqhGD9vUaPf32Yj5","authorship_tag":"ABX9TyPmYktpWzS8Z+EWCIFn/+y1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Pagerank (Tugas 4)**"],"metadata":{"id":"Cj2R_pZdS6ZG"}},{"cell_type":"markdown","source":["Pagerank adalah algoritma yang digunakan dalam analisis jaringan dan pengurutan halaman web. Algoritma Pagerank mengukur kepentingan relatif dari setiap halaman web dalam suatu jaringan dengan menganalisis jumlah dan kualitas tautan masuk ke halaman tersebut dari halaman web lain dalam jaringan."],"metadata":{"id":"jaEXKzgVW7Jn"}},{"cell_type":"code","source":["!pip install PyPDF2\n","!pip install docx2txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P-OegaZaSnI-","executionInfo":{"status":"ok","timestamp":1681812620667,"user_tz":-420,"elapsed":26810,"user":{"displayName":"20-027_Muhammad Aulia Faqihuddin","userId":"11113054701007770115"}},"outputId":"f133b051-b52a-4137-cf69-58e2981dde6f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting PyPDF2\n","  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing_extensions>=3.10.0.0 in /usr/local/lib/python3.9/dist-packages (from PyPDF2) (4.5.0)\n","Installing collected packages: PyPDF2\n","Successfully installed PyPDF2-3.0.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting docx2txt\n","  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: docx2txt\n","  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3977 sha256=268f5ab7f7a5d8bdce14bdb87573c4c607ddb905b7733cb815df5e18a942d287\n","  Stored in directory: /root/.cache/pip/wheels/40/75/01/e6c444034338bde9c7947d3467807f889123465c2371e77418\n","Successfully built docx2txt\n","Installing collected packages: docx2txt\n","Successfully installed docx2txt-0.8\n"]}]},{"cell_type":"markdown","source":["## Import Library"],"metadata":{"id":"nvokhrYVXaAq"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"ORs6pADESXMf","executionInfo":{"status":"ok","timestamp":1681812622705,"user_tz":-420,"elapsed":2054,"user":{"displayName":"20-027_Muhammad Aulia Faqihuddin","userId":"11113054701007770115"}}},"outputs":[],"source":["import numpy as np\n","import requests\n","import io\n","import PyPDF2\n","import docx2txt\n","import sys\n","from nltk.tokenize.punkt import PunktSentenceTokenizer\n","import matplotlib.pyplot as plt\n","import networkx as nx\n","from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer"]},{"cell_type":"markdown","source":["## Menentukan Type Document"],"metadata":{"id":"XIX-iCEVXdsk"}},{"cell_type":"markdown","source":["Menentukan tipe dokumen yang akan dibaca, apakah dalam format .txt atau .pdf, atau tidak valid.\n","\n","Jika tipe dokumen yang dibaca adalah dalam format .txt, maka fungsi akan membuka file dan membaca isi dari file tersebut menggunakan fungsi open() dan read(). \n","Jika tipe dokumen yang dibaca adalah dalam format .pdf, maka fungsi akan membuka file, membaca halaman pertama dari file tersebut menggunakan PyPDF2, dan mengekstrak teks dari halaman tersebut.\n","\n","Jika tipe dokumen yang dibaca tidak valid, fungsi akan mencetak pesan kesalahan dan mengembalikan string kosong.\n","\n","Setelah fungsi berhasil membaca isi dokumen, maka isi dokumen tersebut akan dijadikan sebagai output dari fungsi readDoc(name).\n"],"metadata":{"id":"fQ0F0dBMgWan"}},{"cell_type":"code","source":["# we are going to show an example of how the method is working\n","# first let's take the document as an input\n","def readDoc(name):\n","\n","    # now read the type of document\n","    if name.lower().endswith('.txt'):\n","        choice = 1\n","    elif name.lower().endswith('.pdf'):\n","        choice = 2\n","    else:\n","        choice = 3\n","        # print(name)\n","    # print(choice)\n","    # Case 1: if it is a .txt file\n","        \n","    if choice == 1:\n","        f = open(name, 'r', encoding=\"utf8\")\n","        document = f.read()\n","        f.close()\n","            \n","    # Case 2: if it is a .pdf file\n","    elif choice == 2:\n","        pdfFileObj = open(name, 'rb', encoding=\"utf8\")\n","        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n","        pageObj = pdfReader.getPage(0)\n","        document = pageObj.extractText()\n","        pdfFileObj.close()\n","    \n","    # Case 3: none of the format\n","    else:\n","        print('Failed to load a valid file')\n","        print('Returning an empty string')\n","        document = ''\n","    \n","    # print(type(document))\n","    return document"],"metadata":{"id":"RWt3bc6oTC5s","executionInfo":{"status":"ok","timestamp":1681812622707,"user_tz":-420,"elapsed":24,"user":{"displayName":"20-027_Muhammad Aulia Faqihuddin","userId":"11113054701007770115"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Memecah Kalimat"],"metadata":{"id":"S-l9AZuAamsN"}},{"cell_type":"markdown","source":["Membuat sebuah objek doc_tokenizer dari kelas PunktSentenceTokenizer. Kelas ini merupakan bagian dari library nltk dan digunakan untuk memecah teks menjadi kalimat-kalimat.\n","\n","Memecah teks dokumen menjadi kalimat-kalimat menggunakan method tokenize() dari objek doc_tokenizer. Method ini akan mengembalikan sebuah list yang berisi semua kalimat dalam dokumen.\n","\n","Setelah fungsi berhasil memecah dokumen menjadi kalimat-kalimat, maka list kalimat tersebut akan dijadikan sebagai output dari fungsi tokenize(document).\n"],"metadata":{"id":"NRxo02KJgFKO"}},{"cell_type":"code","source":["def tokenize(document):\n","    # We are tokenizing using the PunktSentenceTokenizer\n","    # we call an instance of this class as sentence_tokenizer\n","    doc_tokenizer = PunktSentenceTokenizer()\n","    \n","    # tokenize() method: takes our document as input and returns a list of all the sentences in the document\n","    \n","    # sentences is a list containing each sentence of the document as an element\n","    sentences_list = doc_tokenizer.tokenize(document)\n","    return sentences_list"],"metadata":{"id":"d9ZrZJRcTMY0","executionInfo":{"status":"ok","timestamp":1681812622708,"user_tz":-420,"elapsed":20,"user":{"displayName":"20-027_Muhammad Aulia Faqihuddin","userId":"11113054701007770115"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Memproses Document"],"metadata":{"id":"E40azDHHb6YD"}},{"cell_type":"markdown","source":["Untuk memproses teks document dapat menggunakan teknik CountVectorizer. CountVectorizer adalah sebuah metode dalam pemrosesan bahasa alami yang digunakan untuk mengubah teks menjadi vektor frekuensi kata.\n","\n","Untuk memproses text document terdapat beberapa langkah-langkah, yaitu:\n","\n","\n","1.   Membaca dokumen dari file \"berita1.txt\" menggunakan fungsi readDoc() dan menyimpannya dalam variabel document.\n","2.   Memecah dokumen menjadi kalimat-kalimat menggunakan fungsi tokenize() dan menyimpannya dalam variabel sentences_list.\n","3.   Membuat sebuah objek cv dari kelas CountVectorizer().\n","4.   Memproses kalimat-kalimat dalam sentences_list menggunakan method fit_transform() dari objek cv. Method ini akan mengubah teks menjadi matriks frekuensi kata, yaitu cv_matrix.\n","\n","\n","Setelah semua proses selesai, dapat menggunakan matriks cv_matrix untuk melakukan analisis teks lebih lanjut.\n","\n"],"metadata":{"id":"-XqrR-1hfMrR"}},{"cell_type":"code","source":["# document = readDoc(\"https://raw.githubusercontent.com/Shakunni/Extractive-Text-Summarization/master/story1.txt\")\n","\n","url = \"https://raw.githubusercontent.com/masaul/data-csv/main/news.txt\"\n","response = requests.get(url)\n","document = io.StringIO(response.text).read()\n","sentences_list = tokenize(document)\n","\n","cv = CountVectorizer()\n","cv_matrix = cv.fit_transform(sentences_list)"],"metadata":{"id":"Vnb1qAovTqE3","executionInfo":{"status":"ok","timestamp":1681812623215,"user_tz":-420,"elapsed":524,"user":{"displayName":"20-027_Muhammad Aulia Faqihuddin","userId":"11113054701007770115"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Menghitung PageRank Setiap Kalimat"],"metadata":{"id":"JXijkaAGcEoc"}},{"cell_type":"markdown","source":["Untuk menghitung PageRank dari setiap kalimat dalam dokumen menggunakan algoritma PageRank dari library NetworkX dibutuhkan langkah-langkah seperti berikut:\n","\n","1.   Menghitung matriks normalisasi TF-IDF dari matriks frekuensi kata cv_matrix menggunakan fungsi TfidfTransformer().fit_transform(cv_matrix) dan menyimpan hasilnya dalam variabel normal_matrix.\n","2.   Menghitung matriks similarity antar kalimat dengan mengalikan normal_matrix dengan transpose-nya menggunakan code res_graph = normal_matrix * normal_matrix.T.\n","3.   Mengubah matriks similarity menjadi sebuah graph menggunakan nx.from_scipy_sparse_array(res_graph) dari NetworkX. Graph ini akan digunakan sebagai input algoritma PageRank.\n","4.   Menghitung PageRank untuk setiap kalimat dalam graph menggunakan nx.pagerank(nx_graph) dan menyimpan hasilnya dalam variabel ranks.\n","5.   Mengurutkan kalimat-kalimat berdasarkan PageRank-nya dari yang tertinggi ke terendah menggunakan fungsi sorted() dan menyimpannya dalam variabel sentence_array. Variabel ini berisi tuple yang terdiri dari nilai PageRank dan kalimat yang bersangkutan.\n","6.   Mengonversi sentence_array menjadi array NumPy menggunakan np.asarray(sentence_array).\n","\n","\n","Setelah proses tersebut dieksekusi maka  variabel sentence_array akan berisi kalimat-kalimat dalam dokumen yang sudah diurutkan berdasarkan PageRank-nya. Kalimat-kalimat pada bagian atas array merupakan kalimat yang dianggap paling penting dalam dokumen"],"metadata":{"id":"1X29QBc5d6S7"}},{"cell_type":"code","source":["normal_matrix = TfidfTransformer().fit_transform(cv_matrix)\n","res_graph = normal_matrix * normal_matrix.T\n","# plt.spy(res_graph)\n","nx_graph = nx.from_scipy_sparse_array(res_graph)\n","ranks = nx.pagerank(nx_graph)\n","\n","sentence_array = sorted(((ranks[i], s) for i, s in enumerate(sentences_list)), reverse=True)\n","sentence_array = np.asarray(sentence_array)"],"metadata":{"id":"yN_6x81jTr8u","executionInfo":{"status":"ok","timestamp":1681812623217,"user_tz":-420,"elapsed":25,"user":{"displayName":"20-027_Muhammad Aulia Faqihuddin","userId":"11113054701007770115"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## Ringkasan Kalimat dari PageRank"],"metadata":{"id":"VOAZuqVccKrF"}},{"cell_type":"markdown","source":["Kemudian untuk memilih kalimat-kalimat yang akan menjadi ringkasan dokumen berdasarkan nilai PageRank mempunyai beberapa langkah-langkah, yaitu :\n","\n","\n","1.   Mencari nilai PageRank tertinggi (rank_max) dan terendah (rank_min) dari sentence_array.\n","\n","2.   Membuat sebuah array kosong temp_array.\n","\n","3.   Jika semua kalimat memiliki nilai PageRank yang sama, artinya semua kalimat dianggap sama pentingnya dan ringkasan dihasilkan dari kalimat pertama dalam dokumen.\n","\n","4.   Jika tidak, untuk setiap kalimat dalam sentence_array, normalisasi nilai PageRank dengan membaginya dengan selisih antara rank_max dan rank_min. Hasilnya kemudian disimpan dalam temp_array.\n","\n","5.   Mencari nilai ambang (threshold) yang digunakan untuk memilih kalimat-kalimat penting yang akan menjadi bagian dari ringkasan. Nilai ambang dihitung dengan mengambil rata-rata dari semua nilai dalam temp_array dan menambahkannya dengan nilai 0,2.\n","\n","6.   Jika temp_array berisi lebih dari satu nilai, memilih kalimat-kalimat yang memiliki nilai PageRank lebih besar dari threshold dan menyimpannya dalam variabel sentence_list. Jika temp_array hanya berisi satu nilai, maka kalimat tersebut langsung dijadikan ringkasan dokumen.\n","\n","7.   Mengembalikan variabel sentence_list yang berisi kalimat-kalimat penting yang telah dipilih sebagai ringkasan dokumen.\n","\n","\n","Setelah proses tersebut di jalankan,  variabel sentence_list akan berisi kalimat-kalimat penting yang telah dipilih sebagai ringkasan dokumen\n","\n"],"metadata":{"id":"LO08vKeUdHQw"}},{"cell_type":"code","source":["rank_max = float(sentence_array[0][0])\n","rank_min = float(sentence_array[len(sentence_array) - 1][0])\n","\n","temp_array = []\n","\n","# if all sentences have equal ranks, means they are all the same\n","# taking any sentence will give the summary, say the first sentence\n","flag = 0\n","if rank_max - rank_min == 0:\n","    temp_array.append(0)\n","    flag = 1\n","\n","# If the sentence has different ranks\n","if flag != 1:\n","    for i in range(0, len(sentence_array)):\n","        temp_array.append((float(sentence_array[i][0]) - rank_min) / (rank_max - rank_min))\n","threshold = (sum(temp_array) / len(temp_array)) + 0.2\n","sentence_list = []\n","if len(temp_array) > 1:\n","    for i in range(0, len(temp_array)):\n","        if temp_array[i] > threshold:\n","                sentence_list.append(sentence_array[i][1])\n","else:\n","    sentence_list.append(sentence_array[0][1])"],"metadata":{"id":"1I_42h7TT4Rn","executionInfo":{"status":"ok","timestamp":1681812623218,"user_tz":-420,"elapsed":22,"user":{"displayName":"20-027_Muhammad Aulia Faqihuddin","userId":"11113054701007770115"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Hasil Kalimat"],"metadata":{"id":"3SynL40IcXvD"}},{"cell_type":"markdown","source":["Didalam loop for, setiap kalimat dalam variabel sentence_list dicetak satu per satu menggunakan perintah print(i).\n","\n","Jika diinginkan, variabel sentence_list juga dapat diubah menjadi sebuah string dengan menggunakan perintah \" \".join(str(x) for x in sentence_list), dimana kalimat-kalimat dalam variabel sentence_list akan dihubungkan menggunakan spasi sehingga membentuk sebuah ringkasan dokumen yang utuh dalam bentuk string. Namun, pada code tersebut, opsi ini tidak digunakan dan digantikan dengan mencetak kalimat-kalimat satu per satu."],"metadata":{"id":"CmmNjpqMcZju"}},{"cell_type":"code","source":["# summary = \" \".join(str(x) for x in sentence_list)\n","# print(summary)\n","\n","for i in sentence_list:\n","    print(i)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3jEiwEsBT9Ip","executionInfo":{"status":"ok","timestamp":1681812623220,"user_tz":-420,"elapsed":21,"user":{"displayName":"20-027_Muhammad Aulia Faqihuddin","userId":"11113054701007770115"}},"outputId":"f471436b-e949-4b43-d094-b1bbb97bb59d"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Produk ini merupakan smartphone terbaru dari Microsoft yang dilengkapi dengan fitur ganda dan berjalan pada sistem operasi Windows 11.\n","Diharapkan dengan hadirnya produk terbaru ini, Microsoft dapat bersaing di pasar smartphone yang semakin kompetitif dan meningkatkan kepercayaan pengguna terhadap produk-produk terbarunya.\n"]}]}]}